#!/bin/bash
STREAMING_TYPE=streaming
MAPRED_JOB_NAME=${JOB_NAME_PREFIX}_calc_ctr

__SELECT_DAYS__=7
function gen_doc_input() {
    local cjv_dir=${HDFS_WORK_PATH}/cjv
    local misscnt=0
    local doc_input_paths=""
    for ((i=1;i<=${__SELECT_DAYS__};i++)); do
        local datestr=`date +%Y%m%d -d "-$i days"`
        local path=${cjv_dir}/${datestr}
        ${HADOOP_BIN} dfs -test -d ${path}
        if [ $? -eq 0 ]; then
            if [ -z ${doc_input_paths} ]; then
                doc_input_paths=${path}/part-*
            else
                doc_input_paths=${doc_input_paths},${path}/part-*
            fi
        else
            ((misscnt++))
        fi
    done

    if [ $misscnt -gt 0 ]; then
        if [ -z ${doc_input_paths} ]; then
            doc_input_paths=${cjv_dir}/history/part-*
        else
            doc_input_paths=${doc_input_paths},${cjv_dir}/history/part-*
        fi
    fi
    echo ${doc_input_paths}
}

MAPRED_INPUT_PATH=`gen_doc_input`
MAPRED_OUTPUT_PATH=${HDFS_WORK_PATH}/ctr/${DATE_FLAG}
MAPRED_TMP_PATH=${HDFS_TMP_PATH}

MAPPER_CMD="python ctr_filter.py ${__SELECT_DAYS__}"
REDUCER_CMD="python user_zip_ctr.py"

# format
INPUT_FORMAT="org.apache.hadoop.mapred.TextInputFormat"
OUTPUT_FORMAT="org.apache.hadoop.mapred.TextOutputFormat"

# upload files
CACHE_ARCHIVES=()
UPLOAD_FILES=("${LOCAL_BIN_PATH}/ctr_filter.py" "${LOCAL_BIN_PATH}/user_zip_ctr.py" "${LOCAL_BIN_PATH}/utils.py")

# task num
MAP_TASKS=300
MAP_TASKS_CAPACITY=300
REDUCE_TASKS=100
REDUCE_TASKS_CAPACITY=100
JOB_PRIORITY=NORMAL
#JOB_QUEUE=profile

# other arguments
ARGUMENTS="-D mapred.job.reduce.memory.mb=3000 -D mapreduce.reduce.java.opts=-Xmx2000m -D mapred.reduce.slowstart.completed.maps=0.95 -D mapreduce.jobtracker.split.metainfo.maxsize=-1 -D mapreduce.input.fileinputformat.split.minsize=671088640 -D mapred.max.map.failures.percent=1"
CHECK_OUTPUT="TRUE"
COPY_TO_LOCAL=${LOCAL_DATA_PATH}/ctr/${DATE_FLAG}
