#!/bin/bash
STREAMING_TYPE=streaming
__TASK_KEY__=${__FIELD__}_ctr_${__CALC_DAYS__}
MAPRED_JOB_NAME=${JOB_NAME_PREFIX}_${__TASK_KEY__}_user
CATEGORY_INPUT_DIR=${HDFS_ROOT_PATH}/user_tw_chn
function gen_doc_input() {
    if [ "x"${__SKIP_FIRST_DAY__} == "xTRUE" ]; then
        local start=2
    else
        local start=1
    fi
    
    local doc_input_paths=""
    local misscnt=0
    for ((i=${start};i<=${__CALC_DAYS__};i++)); do
        local datestr=`date +%Y%m%d -d "-$i days"`
        local path=${CATEGORY_INPUT_DIR}/click_category/${datestr}
        ${HDFS_BIN} dfs -test -d ${path}
        if [ $? -eq 0 ]; then
            if [ -z ${doc_input_paths} ]; then
                doc_input_paths=${path}/part-*
            else
                doc_input_paths=${doc_input_paths},${path}/part-*
            fi
        else
            ((misscnt++))
        fi
    done
    echo ${doc_input_paths}
}

MAPRED_INPUT_PATH=`gen_doc_input`

MAPRED_TMP_PATH=${HDFS_TMP_PATH}

MAPPER_CMD="python user_ctr_mapper.py ${__CALC_DAYS__} ${__FIELD__} ${__MIN_PV_TIME__} ${__MAX_PV_TIME__} ${__MIN_CV_TIME__} ${__MAX_CV_TIME__}"
REDUCER_CMD="python user_ctr_reducer.py ${__CHECK_THRESHOLD__} ${__CLICK_THRESHOLD__} ${__UNIT_DECAY_COEF__}"

# format
INPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileInputFormat"
OUTPUT_FORMAT="org.apache.hadoop.mapred.TextOutputFormat"

# upload files
CACHE_ARCHIVES=()
UPLOAD_FILES=("${LOCAL_BIN_PATH}/user_ctr_mapper.py" "${LOCAL_BIN_PATH}/user_ctr_reducer.py" "${LOCAL_BIN_PATH}/utils.py" "${LOCAL_BIN_PATH}/action.py")

# task num
MAP_TASKS=300
MAP_TASKS_CAPACITY=300
REDUCE_TASKS=100
REDUCE_TASKS_CAPACITY=100
JOB_PRIORITY=VERY_HIGH
#JOB_QUEUE=midway

# other arguments
ARGUMENTS="-D mapred.job.reduce.memory.mb=2000 -D mapred.reduce.slowstart.completed.maps=0.95 -D mapreduce.jobtracker.split.metainfo.maxsize=-1"
CHECK_OUTPUT="TRUE"
#COPY_TO_LOCAL="${LOCAL_DATA_PATH}/${__TASK_KEY__}/${DATE_FLAG}/user"
