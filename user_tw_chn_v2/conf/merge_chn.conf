#!/bin/bash
PREV_DATE=$(date --date="${DATE_FLAG} -1 day" +%Y%m%d)
#PREV_DATE2=$(date --date="${DATE_FLAG} -2 day" +%Y%m%d)
STREAMING_TYPE=streaming
MAPRED_JOB_NAME=${JOB_NAME_PREFIX}_time_decay_${DATE_FLAG}
MAPRED_INPUT_PATH=${HDFS_WORK_PATH}/chn_sc/${DATE_FLAG}/part-*,${HDFS_WORK_PATH}/time_decay_history/${PREV_DATE}/part-*
#MAPRED_INPUT_PATH=${HDFS_WORK_PATH}/chn_sc/${DATE_FLAG}/part-*,${HDFS_WORK_PATH}/time_decay_history/${PREV_DATE2}/part-*
MAPRED_OUTPUT_PATH=${HDFS_WORK_PATH}/time_decay_history/${DATE_FLAG}
MAPRED_TMP_PATH=${HDFS_TMP_PATH}

MAPPER_CMD="python decay_mapper.py"
REDUCER_CMD="python merge_reducer.py"

# format
INPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileInputFormat"
OUTPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileOutputFormat"

# upload files
CACHE_ARCHIVES=()
UPLOAD_FILES=("${LOCAL_BIN_PATH}/decay_mapper.py" "${LOCAL_BIN_PATH}/merge_reducer.py" "${LOCAL_BIN_PATH}/utils.py")

# task num
MAP_TASKS=300
MAP_TASKS_CAPACITY=300
REDUCE_TASKS=100
REDUCE_TASKS_CAPACITY=100
JOB_PRIORITY=VERY_HIGH
#JOB_QUEUE=profile

# other arguments
ARGUMENTS="-D mapred.max.map.failures.percent=1 -D mapred.reduce.slowstart.completed.maps=0.95 -D mapreduce.jobtracker.split.metainfo.maxsize=-1 -D mapred.job.reduce.memory.mb=2500 -D mapreduce.reduce.java.opts=-Xmx2000m"
CHECK_OUTPUT="TRUE"
COMPRESS_OUTPUT="TRUE"
