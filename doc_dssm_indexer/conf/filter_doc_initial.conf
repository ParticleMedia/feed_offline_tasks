#!/bin/bash
STREAMING_TYPE=streaming
MAPRED_JOB_NAME=${JOB_NAME_PREFIX}_filter_doc
MAPRED_INPUT_PATH=/user/services/cpp_new/dump/${PROCESSING_DOC_DATE_FLAG}/${PROCESSING_HOUR_FLAG}/hdfs-*
MAPRED_OUTPUT_PATH=${HDFS_WORK_PATH}/doc/${PROCESSING_DATE_FLAG}/${PROCESSING_HOUR_FLAG}
MAPRED_TMP_PATH=${HDFS_TMP_PATH}

MAPPER_CMD="python filter_docs.py"
REDUCER_CMD="cat"

# format
INPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileInputFormat"
OUTPUT_FORMAT="org.apache.hadoop.mapred.TextOutputFormat"

# upload files
CACHE_ARCHIVES=()
UPLOAD_FILES=("${LOCAL_BIN_PATH}/filter_docs.py")

# task num
MAP_TASKS=300
MAP_TASKS_CAPACITY=300
REDUCE_TASKS=1
REDUCE_TASKS_CAPACITY=1
JOB_PRIORITY=VERY_HIGH
JOB_QUEUE=offline

# other arguments
ARGUMENTS="-D mapred.reduce.slowstart.completed.maps=0.95 -D mapreduce.jobtracker.split.metainfo.maxsize=-1 -D mapreduce.input.fileinputformat.input.dir.recursive=true"
CHECK_OUTPUT="TRUE"
COPY_TO_LOCAL=${LOCAL_DATA_PATH}/docs/${PROCESSING_DATE_FLAG}${PROCESSING_HOUR_FLAG}
