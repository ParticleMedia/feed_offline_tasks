#!/bin/bash
#process 89 days of click category data with time decay
#mapper parameters
__MIN_PV_TIME__=2000
__MAX_PV_TIME__=600000
__MIN_CV_TIME__=2000
__MAX_CV_TIME__=600000
__CALC_DAYS__=90

#reducer parameters
__UNIT_DECAY_COEF__=FALSE
__CHECK_THRESHOLD__=0
__CLICK_THRESHOLD__=0


#
STREAMING_TYPE=streaming
MAPRED_JOB_NAME=${JOB_NAME_PREFIX}_process_89_sc_${__FIELD__}

PREV_DATE=$(date --date="${DATE_FLAG} -1 day" +%Y%m%d)

MAPRED_INPUT_PATH=${HDFS_WORK_PATH}/click_category/history
MAPRED_OUTPUT_PATH=${HDFS_WORK_PATH}/time_decay_history_init/${__FIELD__}/${PREV_DATE}
MAPRED_TMP_PATH=${HDFS_TMP_PATH}

MAPPER_CMD="python user_ctr_mapper.py ${__CALC_DAYS__} ${__FIELD__} ${__SRC_FIELD__} ${__MIN_PV_TIME__} ${__MAX_PV_TIME__} ${__MIN_CV_TIME__} ${__MAX_CV_TIME__}"
REDUCER_CMD="python user_ctr_reducer.py ${__CHECK_THRESHOLD__} ${__CLICK_THRESHOLD__} ${__UNIT_DECAY_COEF__} ${__SRC_FIELD__}"


# format
INPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileInputFormat"
OUTPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileOutputFormat"

# upload files
CACHE_ARCHIVES=()
UPLOAD_FILES=("${LOCAL_BIN_PATH}/user_ctr_mapper.py" "${LOCAL_BIN_PATH}/user_ctr_reducer.py" "${LOCAL_BIN_PATH}/utils.py" "${LOCAL_BIN_PATH}/action.py")

# task num
MAP_TASKS=300
MAP_TASKS_CAPACITY=300
REDUCE_TASKS=100
REDUCE_TASKS_CAPACITY=100
JOB_PRIORITY=VERY_HIGH
#JOB_QUEUE=midway

# other arguments
ARGUMENTS="-D mapred.job.reduce.memory.mb=2000 -D mapred.reduce.slowstart.completed.maps=0.95 -D mapreduce.jobtracker.split.metainfo.maxsize=-1"
CHECK_OUTPUT="TRUE"
COMPRESS_OUTPUT="TRUE"


