#!/bin/bash
STREAMING_TYPE=streaming
MAPRED_JOB_NAME=${JOB_NAME_PREFIX}_user_active

__SELECT_DAYS__=7
function gen_doc_input() {
    local daily_dir=${HDFS_WORK_PATH}/daily
    local doc_input_paths=""
    for ((i=0;i<${__SELECT_DAYS__};i++)); do
        local datestr=`date +%Y%m%d -d "${DATE_FLAG} -$i days"`
        local path=${daily_dir}/${datestr}/part-*
        if [ -z ${doc_input_paths} ]; then
            doc_input_paths=${path}
        else
            doc_input_paths=${doc_input_paths},${path}
        fi
    done
    echo ${doc_input_paths}
}

MAPRED_INPUT_PATH=`gen_doc_input`
MAPRED_INPUT_PATH_DEBUG=${HDFS_WORK_PATH}/daily/${DATE_FLAG}/part-00000
MAPRED_OUTPUT_PATH=${HDFS_WORK_PATH}/active/${DATE_FLAG}
MAPRED_TMP_PATH=${HDFS_TMP_PATH}

MAPPER_CMD="python merge_mapper.py"
REDUCER_CMD="python user_active_merger.py"

# format
INPUT_FORMAT="org.apache.hadoop.mapred.TextInputFormat"
OUTPUT_FORMAT="org.apache.hadoop.mapred.TextOutputFormat"

# upload files
CACHE_ARCHIVES=()
UPLOAD_FILES=("${LOCAL_BIN_PATH}/merge_mapper.py" "${LOCAL_BIN_PATH}/user_active_merger.py" "${LOCAL_BIN_PATH}/utils.py")

# task num
MAP_TASKS=300
MAP_TASKS_CAPACITY=300
REDUCE_TASKS=100
REDUCE_TASKS_CAPACITY=100
JOB_PRIORITY=VERY_HIGH

# other arguments
ARGUMENTS="-D mapred.reduce.slowstart.completed.maps=0.6"
CHECK_OUTPUT="TRUE"
#COPY_TO_LOCAL=${LOCAL_DATA_PATH}/active/${DATE_FLAG}
